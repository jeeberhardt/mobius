{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcb8d918",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import gpytorch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import umap.umap_ as umap\n",
    "from scipy.stats import norm\n",
    "from botorch.models.gpytorch import GPyTorchModel\n",
    "from botorch.fit import fit_gpytorch_model\n",
    "from botorch.utils.transforms import standardize, normalize, unnormalize\n",
    "from botorch.acquisition import ExpectedImprovement, UpperConfidenceBound, ProbabilityOfImprovement\n",
    "from scipy import linalg\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem.Fingerprints import FingerprintMols\n",
    "from rdkit.Chem import PathToSubmol, FindAtomEnvironmentOfRadiusN, MolToSmiles\n",
    "from rdkit.Chem.AtomPairs import Pairs\n",
    "from MDAnalysis.analysis.rms import rmsd\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from map4 import MAP4Calculator\n",
    "#import tmap as tm\n",
    "\n",
    "\n",
    "#from mobius import ForceField, VirtualTarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dab7e354",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TanimotoSimilarityKernel(gpytorch.kernels.Kernel):\n",
    "    # the sequence kernel is stationary\n",
    "    is_stationary = True     \n",
    "\n",
    "    # this is the kernel function\n",
    "    def forward(self, x1, x2, diag=False, last_dim_is_batch=False, **params):\n",
    "        if last_dim_is_batch:\n",
    "            # Not tested\n",
    "            x1 = x1.transpose(-1, -2).unsqueeze(-1)\n",
    "            x2 = x2.transpose(-1, -2).unsqueeze(-1)\n",
    "\n",
    "        x1_eq_x2 = torch.equal(x1, x2)\n",
    "        \n",
    "        x1s = torch.sum(torch.square(x1), dim=-1)\n",
    "        x2s = torch.sum(torch.square(x2), dim=-1)\n",
    "        \n",
    "        if diag:\n",
    "            if x1_eq_x2:\n",
    "                res = torch.ones(*x1.shape[:-2], x1.shape[-2], dtype=x1.dtype, device=x1.device)\n",
    "                return res\n",
    "            else:\n",
    "                product = torch.mul(x1, x2).sum(dim=1)\n",
    "                denominator = torch.add(x2s, x1s) - product\n",
    "        else:\n",
    "            product = torch.mm(x1, x2.transpose(1, 0))\n",
    "            denominator = torch.add(x2s, x1s[:, None]) - product\n",
    "\n",
    "        res = product / denominator\n",
    "        \n",
    "        return res\n",
    "\n",
    "    \n",
    "# We will use the simplest form of GP model, exact inference\n",
    "class ExactGPModel(gpytorch.models.ExactGP, GPyTorchModel):\n",
    "    # to inform GPyTorchModel API\n",
    "    _num_outputs = 1\n",
    "    \n",
    "    def __init__(self, train_x, train_y, likelihood, kernel=None):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "\n",
    "        if kernel is not None:\n",
    "            self.covar_module = gpytorch.kernels.ScaleKernel(kernel())\n",
    "        else:\n",
    "            self.covar_module = gpytorch.kernels.ScaleKernel(TanimotoSimilarityKernel())\n",
    "\n",
    "        # make sure we're on the right device/dtype\n",
    "        self.to(train_x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "    \n",
    "def get_fitted_model(train_x, train_y, state_dict=None, kernel=None):\n",
    "    # initialize and fit model\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = ExactGPModel(train_x, train_y, likelihood, kernel)\n",
    "    \n",
    "    if state_dict is not None:\n",
    "        model.load_state_dict(state_dict)\n",
    "    \n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    mll.to(train_x)\n",
    "    \n",
    "    # Train model!\n",
    "    fit_gpytorch_model(mll)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "791821b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, likelihood, test_x):\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    \n",
    "    # Make predictions by feeding model through likelihood\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        # Test points are regularly spaced along [0,1]\n",
    "        return likelihood(model(test_x))\n",
    "\n",
    "\n",
    "def plot(y_test, observed_pred, fig_filename=None):\n",
    "    with torch.no_grad():\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "        \n",
    "        ax.scatter(y_test, observed_pred)\n",
    "        \n",
    "        ax.set_xlim([np.min([y_test, observed_pred]) - 1, np.max([y_test, observed_pred]) + 1])\n",
    "        ax.set_ylim([np.min([y_test, observed_pred]) - 1, np.max([y_test, observed_pred]) + 1])\n",
    "        ax.set_xlabel('Experimental values (kcal/mol)', fontsize=20)\n",
    "        ax.set_ylabel('Predicted values (kcal/mol)', fontsize=20)\n",
    "        \n",
    "        if fig_filename is not None:\n",
    "            plt.savefig(fig_filename, bbox_inches='tight', dpi=300)\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bad36f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_improvement(model, Y_train, Xsamples, greater_is_better=False, xi=0.00):\n",
    "    \"\"\" expected_improvement\n",
    "    Expected improvement acquisition function.\n",
    "    \n",
    "    Source: https://github.com/thuijskens/bayesian-optimization/blob/master/python/gp.py\n",
    "    \n",
    "    Arguments:\n",
    "    ----------\n",
    "        model: Gaussian process model\n",
    "        Y_train: Array that contains all the observed energy interaction seed so far\n",
    "        X_samples: Samples we want to try out\n",
    "        greater_is_better: Indicates whether the loss function is to be maximised or minimised.\n",
    "        xi: Exploitation-exploration trade-off parameter\n",
    "\n",
    "    \"\"\"\n",
    "    # calculate mean and stdev via surrogate function\n",
    "    observed_pred = predict(model, model.likelihood, Xsamples)\n",
    "    sigma = observed_pred.variance.sqrt().detach().numpy()\n",
    "    mu = observed_pred.mean.detach().numpy()\n",
    "\n",
    "    if greater_is_better:\n",
    "        loss_optimum = np.max(Y_train.numpy())\n",
    "    else:\n",
    "        loss_optimum = np.min(Y_train.numpy())\n",
    "\n",
    "    scaling_factor = (-1) ** (not greater_is_better)\n",
    "\n",
    "    # calculate the expected improvement\n",
    "    Z = scaling_factor * (mu - loss_optimum - xi) / (sigma + 1E-9)\n",
    "    ei = scaling_factor * (mu - loss_optimum) * norm.cdf(Z) + (sigma * norm.pdf(Z))\n",
    "    ei[sigma == 0.0] == 0.0\n",
    "\n",
    "    return -1 * ei\n",
    "\n",
    "\n",
    "# probability of improvement acquisition function\n",
    "def probability_of_improvement(model, Y_train, Xsamples, greater_is_better=False):\n",
    "    \"\"\" probability_of_improvement\n",
    "    Probability of improvement acquisition function.\n",
    "        \n",
    "    Arguments:\n",
    "    ----------\n",
    "        model: Gaussian process model\n",
    "        Y_train: Array that contains all the observed energy interaction seed so far\n",
    "        X_samples: Samples we want to try out\n",
    "        greater_is_better: Indicates whether the loss function is to be maximised or minimised.\n",
    "\n",
    "    \"\"\"\n",
    "    # calculate mean and stdev via surrogate function\n",
    "    observed_pred = predict(model, model.likelihood, Xsamples)\n",
    "    sigma = observed_pred.variance.sqrt().detach().numpy()\n",
    "    mu = observed_pred.mean.detach().numpy()\n",
    "    \n",
    "    if greater_is_better:\n",
    "        loss_optimum = np.max(Y_train.numpy())\n",
    "    else:\n",
    "        loss_optimum = np.min(Y_train.numpy())\n",
    "\n",
    "    scaling_factor = (-1) ** (not greater_is_better)\n",
    "    \n",
    "    # calculate the probability of improvement\n",
    "    Z = scaling_factor * (mu - loss_optimum) / (sigma + 1E-9)\n",
    "    pi = norm.cdf(Z)\n",
    "    pi[sigma == 0.0] == 0.0\n",
    "    \n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc25f07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import defaultdict\n",
    "\n",
    "import tmap as tm\n",
    "from mhfp.encoder import MHFPEncoder\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdmolops\n",
    "from rdkit.Chem.rdmolops import GetDistanceMatrix\n",
    "\n",
    "\n",
    "def to_smiles(mol):\n",
    "    return Chem.MolToSmiles(mol, canonical=True, isomericSmiles=False)\n",
    "\n",
    "\n",
    "class MAP4Calculator:\n",
    "\n",
    "    def __init__(self, dimensions=1024, radius=2, is_counted=False, is_folded=False, return_strings=False):\n",
    "        \"\"\"\n",
    "        MAP4 calculator class\n",
    "        \"\"\"\n",
    "        self.dimensions = dimensions\n",
    "        self.radius = radius\n",
    "        self.is_counted = is_counted\n",
    "        self.is_folded = is_folded\n",
    "        self.return_strings = return_strings\n",
    "\n",
    "        if self.is_folded:\n",
    "            self.encoder = MHFPEncoder(dimensions)\n",
    "        else:\n",
    "            self.encoder = tm.Minhash(dimensions)\n",
    "\n",
    "    def calculate(self, mol):\n",
    "        \"\"\"Calculates the atom pair minhashed fingerprint\n",
    "        Arguments:\n",
    "            mol -- rdkit mol object\n",
    "        Returns:\n",
    "            tmap VectorUint -- minhashed fingerprint\n",
    "        \"\"\"\n",
    "        \n",
    "        atom_env_pairs = self._calculate(mol)\n",
    "        if self.is_folded:\n",
    "            return self._fold(atom_env_pairs)\n",
    "        elif self.return_strings:\n",
    "            return atom_env_pairs\n",
    "        return self.encoder.from_string_array(atom_env_pairs)\n",
    "\n",
    "    def calculate_many(self, mols):\n",
    "        \"\"\" Calculates the atom pair minhashed fingerprint\n",
    "        Arguments:\n",
    "            mols -- list of mols\n",
    "        Returns:\n",
    "            list of tmap VectorUint -- minhashed fingerprints list\n",
    "        \"\"\"\n",
    "\n",
    "        atom_env_pairs_list = [self._calculate(mol) for mol in mols]\n",
    "        if self.is_folded:\n",
    "            return [self._fold(pairs) for pairs in atom_env_pairs_list]\n",
    "        elif self.return_strings:\n",
    "            return atom_env_pairs_list\n",
    "        return self.encoder.batch_from_string_array(atom_env_pairs_list)\n",
    "\n",
    "    def _calculate(self, mol):\n",
    "        return self._all_pairs(mol, self._get_atom_envs(mol))\n",
    "    \n",
    "    def counted_fold(self, hash_values, length=2048):\n",
    "        \"\"\"Folds the hash values to a binary vector of a given length.\n",
    "        Arguments:\n",
    "          hash_value {numpy.ndarray} -- An array containing the hash values.\n",
    "          length {int} -- The length of the folded fingerprint (default: {2048})\n",
    "\n",
    "        Returns:\n",
    "          numpy.ndarray -- The folded fingerprint.\n",
    "        \"\"\"\n",
    "        folded = np.zeros(length, dtype=np.uint8)\n",
    "        \n",
    "        positions, counts = np.unique(hash_values % length, return_counts=True)\n",
    "        folded[positions] = counts\n",
    "        \n",
    "        #folded[hash_values % length] += 1\n",
    "\n",
    "        return folded\n",
    "\n",
    "    def _fold(self, pairs):\n",
    "        fp_hash = self.encoder.hash(set(pairs))\n",
    "        \n",
    "        if self.is_counted:\n",
    "            fp_folded = self.counted_fold(fp_hash, self.dimensions)\n",
    "        else:\n",
    "            fp_folded = self.encoder.fold(fp_hash, self.dimensions)\n",
    "        \n",
    "        return fp_folded\n",
    "\n",
    "    def _get_atom_envs(self, mol):\n",
    "        atoms_env = {}\n",
    "        for atom in mol.GetAtoms():\n",
    "            idx = atom.GetIdx()\n",
    "            for radius in range(1, self.radius + 1):\n",
    "                if idx not in atoms_env:\n",
    "                    atoms_env[idx] = []\n",
    "                atoms_env[idx].append(MAP4Calculator._find_env(mol, idx, radius))\n",
    "        return atoms_env\n",
    "\n",
    "    @classmethod\n",
    "    def _find_env(cls, mol, idx, radius):\n",
    "        env = rdmolops.FindAtomEnvironmentOfRadiusN(mol, radius, idx)\n",
    "        atom_map = {}\n",
    "\n",
    "        submol = Chem.PathToSubmol(mol, env, atomMap=atom_map)\n",
    "        if idx in atom_map:\n",
    "            smiles = Chem.MolToSmiles(submol, rootedAtAtom=atom_map[idx], canonical=True, isomericSmiles=False)\n",
    "            return smiles\n",
    "        return ''\n",
    "\n",
    "    def _all_pairs(self, mol, atoms_env):\n",
    "        atom_pairs = []\n",
    "        distance_matrix = GetDistanceMatrix(mol)\n",
    "        num_atoms = mol.GetNumAtoms()\n",
    "        shingle_dict = defaultdict(int)\n",
    "        \n",
    "        for idx1, idx2 in itertools.combinations(range(num_atoms), 2):\n",
    "            dist = str(int(distance_matrix[idx1][idx2]))\n",
    "\n",
    "            for i in range(self.radius):\n",
    "                env_a = atoms_env[idx1][i]\n",
    "                env_b = atoms_env[idx2][i]\n",
    "\n",
    "                ordered = sorted([env_a, env_b])\n",
    "\n",
    "                shingle = '{}|{}|{}'.format(ordered[0], dist, ordered[1])\n",
    "\n",
    "                if self.is_counted:\n",
    "                    shingle_dict[shingle] += 1\n",
    "                    shingle += '|' + str(shingle_dict[shingle])\n",
    "\n",
    "                atom_pairs.append(shingle.encode('utf-8'))\n",
    "        \n",
    "        return list(set(atom_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c829b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map4_fingerprint(sequences, dimensions=4096, radius=2, is_counted=False, is_folded=True):\n",
    "    MAP4_unf = MAP4Calculator(dimensions=dimensions, radius=radius, is_counted=is_counted, is_folded=is_folded)\n",
    "    fps = MAP4_unf.calculate_many([Chem.rdmolfiles.MolFromFASTA(s) for s in sequences])\n",
    "    return torch.from_numpy(np.array(fps)).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7d4f8f",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86536e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mhci = pd.read_csv('binding_data_2013/bdata.20130222.mhci.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d762dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We removed those binding affinity values\n",
    "# A lot of peptides were set with those values. Looks like some default values assigned...\n",
    "dirty_values = [1, 2, 3, 5000, 10000, 20000, 43424, 50000, 69444.44444, 78125]\n",
    "\n",
    "# Split dataset in training and testing sets\n",
    "mhci = mhci[(mhci['mhc_allele'] == 'HLA-A*02:01') &\n",
    "                (~mhci['affinity_binding'].isin(dirty_values))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f6f868",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoys = []\n",
    "\n",
    "with open('uniprot_sprot.fasta') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "    sequence = ''\n",
    "    \n",
    "    for line in lines:\n",
    "        if line.startswith('>'):\n",
    "            if sequence != '':\n",
    "                decoys.append((sequence, len(sequence)))\n",
    "            sequence = ''\n",
    "        else:\n",
    "            sequence += line.strip()\n",
    "        \n",
    "    decoys.append((sequence, len(sequence)))\n",
    "    \n",
    "decoys = pd.DataFrame(data=decoys, columns=('sequence', 'length'))\n",
    "\n",
    "mhci_decoys = decoys[(decoys['length'] >= 5) & (decoys['length'] <= 20)].copy()\n",
    "# Remove peptide with unknow amino acids\n",
    "mhci_decoys = mhci_decoys[~mhci_decoys['sequence'].str.contains('X|Z|B|U', regex=True)]\n",
    "mhci_decoys['energy'] = 0.\n",
    "\n",
    "print('Number of decoy peptides: %d' % mhci_decoys.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81ebb01",
   "metadata": {},
   "source": [
    "### MAP4 counter fingerprint + MinMax kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1126599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(mhci['sequence'], mhci['energy'], test_size=0.3)\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "X_train_fps = map4_fingerprint(X_train, is_counted=True)\n",
    "X_test_fps = map4_fingerprint(X_test, is_counted=True)\n",
    "\n",
    "y_train = torch.from_numpy(y_train.values).float()\n",
    "y_test = torch.from_numpy(y_test.values).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb2ad37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxKernel(gpytorch.kernels.Kernel):\n",
    "    # the sequence kernel is stationary\n",
    "    is_stationary = True     \n",
    "\n",
    "    # this is the kernel function\n",
    "    def forward(self, x1, x2, diag=False, last_dim_is_batch=False, **params):\n",
    "        x1_eq_x2 = torch.equal(x1, x2)\n",
    "        \n",
    "        if diag and x1_eq_x2:\n",
    "            res = torch.ones(*x1.shape[:-2], x1.shape[-2], dtype=x1.dtype, device=x1.device)\n",
    "            return res\n",
    "        \n",
    "        minkernel = torch.zeros(x1.shape[-2], x2.shape[-2], dtype=x1.dtype, device=x1.device)\n",
    "        maxkernel = torch.zeros(x1.shape[-2], x2.shape[-2], dtype=x1.dtype, device=x1.device)\n",
    "     \n",
    "        for d in range(x1.shape[-1]):\n",
    "            minkernel += torch.minimum(x1[:, d, None], x2[:, d])\n",
    "            maxkernel += torch.maximum(x1[:, d, None], x2[:, d])\n",
    "        \n",
    "        res = minkernel / maxkernel\n",
    "        \n",
    "        if diag:\n",
    "            # Lazy and not optimal...\n",
    "            res = res.diag()\n",
    "        \n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53148d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_fitted_model(X_train_fps, y_train, kernel=MinMaxKernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcab258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict energy for test set\n",
    "observed_pred = predict(model, model.likelihood, X_test_fps)\n",
    " \n",
    "print('R2   : %.3f' % pearsonr(y_test, observed_pred.mean.numpy())[0]**2)\n",
    "print('RMSD : %.3f kcal/mol' % rmsd(y_test, observed_pred.mean.numpy()))\n",
    "\n",
    "# Plot results\n",
    "plot(y_test.numpy(), observed_pred.mean.numpy(), fig_filename='correlation_exp_pred_MAP4_counted_fps_minmax_mhc-i_cleaned_all_size_pep.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f1876d",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = MinMaxKernel()\n",
    "m = k.forward(X_train_fps, X_train_fps, diag=True)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d892dd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = TanimotoSimilarityKernel()\n",
    "m = k.forward(X_train_fps[:10], X_train_fps[:10])\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5a1abb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dcdca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509afa15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
