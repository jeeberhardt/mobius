#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Problem
#

import numpy as np
from pymoo.core.problem import Problem


class Problem(Problem):
    """
    Class to define Single/Multi/Many-Objectives SequenceGA problem.
    """

    def __init__(self, polymers, scores, acq_fun, filters=None, **kwargs):
        """
        Initialize the Single/Multi/Many-Objectives SequenceGA problem.

        Parameters
        ----------
        polymers : array-like of str
            Polymers.
        scores : array-like of float or int
            Score associated to each polymer.
        acq_fun : `AcquisitionFunction`
            Acquisition function to be evaluated for each new polymer generated.
        filters : array-like of `Filter`, default=None
            Filters to be applied to each new polymer generated.
        **kwargs : dict
            Additional keyword arguments.

        """
        if filters is None:
            n_ieq_constr = 0
        else:
            n_ieq_constr = len(filters)

        super().__init__(n_var=1, n_obj=acq_fun.number_of_objectives,
                         n_ieq_constr=n_ieq_constr)

        self._prior_data = {p: s for p, s in zip(polymers, scores)}
        self._polymers_cache = {}
        self._acq_fun = acq_fun
        self._filters = filters
        self._pre_evaluation = True

    def _evaluate(self, x, out, *args, **kwargs):
        """
        Function to evaluate performance of each new polymer generated.

        Parameters
        ----------
        x : ndarray of str
            Polymers generated by GA to be evaluated by the acquisition functions
        out : ndarray
            Returning objective functions' scores to be minimised by pymoo.

        """
        polymers = x.ravel()
        pre_evaluation_penalty = 999.

        if self._pre_evaluation:
            # For the first GA generation, we use the experimental scores
            # then we will use the acquisition scores from the surrogate models.
            try:
                # We shift all the experimental scores in the opposite direction
                # by a large number to ensure that the polymers generated during the
                # optimization will be better acquisition scores.
                acq_values = np.array([self._prior_data[p] for p in polymers])
                acq_values += -1. * self._acq_fun.scaling_factors * pre_evaluation_penalty
                if self._filters is not None:
                    # We convert booleans to int, and flip the values, so 0 means feasible, and 1 means infeasible.
                    feasibility = 1 - np.column_stack([f.apply(polymers) for f in self._filters]).astype(int)
            except KeyError:
                msg = f'Some polymers not found in the input experimental data. '
                msg += 'Did you forget to turn on the eval mode?'
                raise RuntimeError(msg)
        else:
            # Evaluate unseen polymer with acquisition scores from surrogate models
            acq_values = self._acq_fun.forward(polymers) * self._acq_fun.scaling_factors
            if self._filters is not None:
                # We convert booleans to int, and flip the values, so 0 means feasible, and 1 means infeasible.
                feasibility = 1 - np.column_stack([f.apply(polymers) for f in self._filters]).astype(int)

        out["F"] = acq_values
        if self._filters is not None:
            out["G"] = feasibility

    def pre_eval(self):
        """
        Function to set pre-evaluation mode on. In pre-evaluation mode, the scores will be
        obtained from the experimental data only.

        """
        self._pre_evaluation = True

    def eval(self):
        """
        Function to set pre-evaluation mode off. In evaluation mode, the scores will be
        obtained from the surrogate models.

        """
        self._pre_evaluation = False
