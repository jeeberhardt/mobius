#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Problem
#

import numpy as np
from pymoo.core.problem import Problem


class Problem(Problem):
    """
    Class to define Single/Multi/Many-Objectives SequenceGA problem.
    """

    def __init__(self, polymers, scores, acq_funs, n_inequality_constr=0, n_equality_constr=0, **kwargs):
        """
        Initialize the Single/Multi/Many-Objectives SequenceGA problem.

        Parameters
        ----------
        acq_funs : `AcquisitionFunction` or list of `AcquisitionFunction` objects
            Acquisition functions to be evaluated for each new polymer generated.
        n_inequality_constr : int, default : 0
            Number of inequality constraints.
        n_equality_constr : int, default : 0
            Number of equality constraints.
        **kwargs : dict
            Additional keyword arguments.

        """
        super().__init__(n_var=1, n_obj=len(acq_funs),
                         n_ieq_constr=n_inequality_constr,
                         n_eq_constr=n_equality_constr)

        self._prior_data = {p: s for p, s in zip(polymers, scores)}
        self._polymers_cache = {}
        if not isinstance(acq_funs, list):
            acq_funs = [acq_funs]
        self._acq_funs = acq_funs
        self._pre_evaluation = True

    def _evaluate(self, x, out, *args, **kwargs):
        """
        Function to evaluate performance of each new polymer generated.

        Parameters
        ----------
        x : ndarray of str
            Polymers generated by GA to be evaluated by the acquisition functions
        out : ndarray
            Returning objective functions' scores to be minimised by pymoo.

        """
        polymers = x.ravel()

        # Keep only unseen polymers. We don't want to reevaluate known polymers...
        to_not_evaluate_idx = np.nonzero(np.in1d(polymers, self._polymers_cache.keys()))[0]
        to_evaluate_idx = np.nonzero(~np.in1d(polymers, self._polymers_cache.keys()))[0]

        # If there is no new polymers, we skip the evaluation
        if to_evaluate_idx.size == 0:
            # But we still need to retrieve the scores of all the known polymers
            scores = np.array([self._polymers_cache[p] for p in polymers])
            scores = scores.reshape(scores.shape[0], -1)
        else:
            # For the first GA generation, we use the experimental scores
            # then we will use the acquisition scores from the surrogate models.
            # In the pre-evaluation mode, the data are not added to the cache.
            if self._pre_evaluation:
                try:
                    scores = np.array([self._prior_data[p] for p in polymers])
                except KeyError:
                    msg = f'Some polymers not found in the input experimental data. '
                    msg += 'Did you forget to turn on the eval mode?'
                    raise RuntimeError(msg)
            else:
                scores = np.zeros((len(polymers), len(self._acq_funs)))

                # Evaluate unseen polymer with acquisition scores from surrogate models
                for i, acq_fun in enumerate(self._acq_funs):
                    predictions = acq_fun.forward(polymers[to_evaluate_idx])
                    scores[to_evaluate_idx, i] = acq_fun.scaling_factor * predictions.acq

                # Complete with scores of already seen polymers
                seen_scores = np.array([self._polymers_cache[p] for p in polymers[to_not_evaluate_idx]])
                if seen_scores.size > 0:
                    scores[to_not_evaluate_idx, :] = seen_scores

                # Record acquisition score for found polymer in cache
                self._polymers_cache.update(dict(zip(polymers[to_evaluate_idx], scores[to_evaluate_idx])))

        out["F"] = scores

    def pre_eval(self):
        """
        Function to set pre-evaluation mode on. In pre-evaluation mode, the scores will be
        obtained from the experimental data only.

        """
        self._pre_evaluation = True

    def eval(self):
        """
        Function to set pre-evaluation mode off. In evaluation mode, the scores will be
        obtained from the surrogate models.

        """
        self._pre_evaluation = False
